[{"authors":[],"categories":["R"],"content":"\rThe coronavirus is halting public live all over the world. So, many of the patterns of data regarding flight departures or traffic that we see these days are, objectively speaking, unsurprising. As of now, most of us are coping well with the situation and stay at home, as is recommended. As a result, the public sphere is emptying. Right now, this is a strategy that I support. However, it is hitting the economy harder than one might think. This is especially true for the small shops in the city centres. Most of them have been forced to close.\nSo, looking at pedestrian data can show us two things, anyway. 1) They show us how persistently people are keeping away from German city centres. And 2) they show us a rough estimate of how bad the situation is for the shops in the city centres. Maybe two years ago, I learned from a colleague that there is this new homepage where one could get pedestrian data from German cities, where they measure the amount of pedestrians on central German shopping streets using lasers. The site is called hystreet.com and its data can be downloaded freely after creating an account. Since the spread of Covid-19, some journalists have had a look at this kind of data already for the big German cities. In this tiny, quick hands-on analysis, I focus on the federal state of Baden-Württemberg, which is where I live.\nFirst, we need a couple of packages.\nlibrary(tidyverse, warn.conflicts = FALSE)\rlibrary(here)\rlibrary(lubridate)\rSecond, we need to do a bit of data wrangling. I start with loading two datasets that I downloaded from hystreet.com. They contain data for the largest shopping street in Stuttgart, the Königstraße. The dataset from 2020 contains data from January 6th until March 26th. I have a second one from 2019 that contains data from March 1st to March 31st.\n# Read the csv file from hystreet.com\rdf1 \u0026lt;- read.csv(here::here(\u0026quot;static/files/stuttgart-königstraße-mitte.csv\u0026quot;), sep = \u0026quot;;\u0026quot;, stringsAsFactors = FALSE)\rdf2 \u0026lt;- read.csv(here::here(\u0026quot;static/files/stuttgart-königstraße-mitte-2019.csv\u0026quot;), sep = \u0026quot;;\u0026quot;, stringsAsFactors = FALSE)\r# Some data preparation\rdf1 \u0026lt;- df1 %\u0026gt;% mutate(time_of_measurement = as.POSIXct(time_of_measurement),\rdate = lubridate::as_date(time_of_measurement),\rtime = lubridate::ceiling_date(time_of_measurement, unit = \u0026quot;hours\u0026quot;))\rdf2 \u0026lt;- df2 %\u0026gt;% mutate(time_of_measurement = as.POSIXct(time_of_measurement),\rdate = lubridate::as_date(time_of_measurement),\rtime = lubridate::ceiling_date(time_of_measurement, unit = \u0026quot;hours\u0026quot;))\rAfter doing so, the data set looks like this:\nglimpse(df1)\r## Observations: 30,934\r## Variables: 7\r## $ location \u0026lt;chr\u0026gt; \u0026quot;KÃ¶nigstraÃŸe (Mitte), Stuttgart\u0026quot;, \u0026quot;KÃ¶nigstra...\r## $ time_of_measurement \u0026lt;dttm\u0026gt; 2020-01-06 00:04:21, 2020-01-06 00:09:56, 2020...\r## $ counted_pedestrians \u0026lt;int\u0026gt; 31, 24, 34, 30, 32, 23, 23, 12, 27, 27, 15, 14,...\r## $ type \u0026lt;chr\u0026gt; \u0026quot;regular\u0026quot;, \u0026quot;regular\u0026quot;, \u0026quot;regular\u0026quot;, \u0026quot;regular\u0026quot;, \u0026quot;re...\r## $ incidents \u0026lt;lgl\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,...\r## $ date \u0026lt;date\u0026gt; 2020-01-06, 2020-01-06, 2020-01-06, 2020-01-06...\r## $ time \u0026lt;dttm\u0026gt; 2020-01-06 01:00:00, 2020-01-06 01:00:00, 2020...\rhead(df1, n = 5)\r## location time_of_measurement counted_pedestrians\r## 1 KÃ¶nigstraÃŸe (Mitte), Stuttgart 2020-01-06 00:04:21 31\r## 2 KÃ¶nigstraÃŸe (Mitte), Stuttgart 2020-01-06 00:09:56 24\r## 3 KÃ¶nigstraÃŸe (Mitte), Stuttgart 2020-01-06 00:14:15 34\r## 4 KÃ¶nigstraÃŸe (Mitte), Stuttgart 2020-01-06 00:19:58 30\r## 5 KÃ¶nigstraÃŸe (Mitte), Stuttgart 2020-01-06 00:24:37 32\r## type incidents date time\r## 1 regular NA 2020-01-06 2020-01-06 01:00:00\r## 2 regular NA 2020-01-06 2020-01-06 01:00:00\r## 3 regular NA 2020-01-06 2020-01-06 01:00:00\r## 4 regular NA 2020-01-06 2020-01-06 01:00:00\r## 5 regular NA 2020-01-06 2020-01-06 01:00:00\rThe first thing I am interested in is the long-term perspective. First restrictions on public life have been announced by the federal government on March 17th (red line in the figure below), further restrictions followed on March 23rd (blue line in the figure below). The second figure shows a comparable time span from 2019. This way, we can (if only slightly) control for seasonal effects, too. That second time span of 2019 serves as a benchmark for how the number of pedestrians would have developed if the coronavirus restrictions would not have hit. I scaled the y axis equally to a maximum of 15,000 pedestrians per hour to spot the difference more easily.\ndf1 %\u0026gt;% filter(date \u0026gt;= \u0026quot;2020-03-01\u0026quot;) %\u0026gt;% group_by(time) %\u0026gt;% summarise(n = sum(counted_pedestrians)) %\u0026gt;%\rggplot(aes(x = time, y = n)) + geom_line(size = 1) +\rgeom_vline(xintercept = as.POSIXct(\u0026quot;2020-03-17\u0026quot;), color = \u0026quot;red\u0026quot;) +\rgeom_vline(xintercept = as.POSIXct(\u0026quot;2020-03-23\u0026quot;), color = \u0026quot;blue\u0026quot;) +\rggtitle(\u0026quot;2020: Pedestrians in the Stuttgart city centre\u0026quot;, subtitle = \u0026quot;Cumulated number on Königstraße (Mitte)\u0026quot;) + labs(caption = \u0026quot;Source: hystreet.com\u0026quot;) + xlab(\u0026quot;Date\u0026quot;) + ylab(\u0026quot;Sum of pedestrians per hour\u0026quot;) +\rylim(0, 15000)\rdf2 %\u0026gt;% filter(date \u0026lt;= \u0026quot;2019-03-26\u0026quot;) %\u0026gt;% group_by(time) %\u0026gt;% summarise(n = sum(counted_pedestrians)) %\u0026gt;%\rggplot(aes(x = time, y = n)) + geom_line(size = 1) +\rggtitle(\u0026quot;2019: Pedestrians in the Stuttgart city centre\u0026quot;, subtitle = \u0026quot;Cumulated number on Königstraße (Mitte)\u0026quot;) + labs(caption = \u0026quot;Source: hystreet.com\u0026quot;) + xlab(\u0026quot;Date\u0026quot;) + ylab(\u0026quot;Sum of pedestrians per hour\u0026quot;) + ylim(0, 15000)\rYou can see that even before the restrictions were passed by the federal government, numbers of pedestrians were already in decline and are now almost absent. In raw numbers, this is (again, compared to 2019):\ndf1 %\u0026gt;% group_by(date) %\u0026gt;% summarise(n = sum(counted_pedestrians)) %\u0026gt;% tail(., n = 7)\r## # A tibble: 7 x 2\r## date n\r## \u0026lt;date\u0026gt; \u0026lt;int\u0026gt;\r## 1 2020-03-20 9908\r## 2 2020-03-21 2884\r## 3 2020-03-22 4350\r## 4 2020-03-23 6628\r## 5 2020-03-24 7564\r## 6 2020-03-25 6692\r## 7 2020-03-26 656\rdf2 %\u0026gt;% group_by(date) %\u0026gt;% summarise(n = sum(counted_pedestrians)) %\u0026gt;% tail(., n = 7)\r## # A tibble: 7 x 2\r## date n\r## \u0026lt;date\u0026gt; \u0026lt;int\u0026gt;\r## 1 2019-03-25 34788\r## 2 2019-03-26 40252\r## 3 2019-03-27 44181\r## 4 2019-03-28 50518\r## 5 2019-03-29 69944\r## 6 2019-03-30 148604\r## 7 2019-03-31 35709\rAs a last analysis, I want to focus on the weekends, unsurprisingly the days with most pedestrians in the city centres. This last figure basically speaks for itself. I compare two weekends end of March, 2020 versus 2019. It is evident 1) how many people stayed home (as long as they did not meet up in any other place instead) and 2) how many potential customers the city centre shops are missing every day (and especially Saturday) that they have to remain closed.\ntime1 \u0026lt;- seq.POSIXt(as.POSIXct(\u0026quot;2020-03-20 00:00:00\u0026quot;), as.POSIXct(\u0026quot;2020-03-23 00:00:00\u0026quot;), by = \u0026quot;hour\u0026quot;)\rtime2 \u0026lt;- seq.POSIXt(as.POSIXct(\u0026quot;2019-03-22 00:00:00\u0026quot;), as.POSIXct(\u0026quot;2019-03-25 00:00:00\u0026quot;), by = \u0026quot;hour\u0026quot;)\rplot1 \u0026lt;- df1 %\u0026gt;%\rfilter(time %in% time1) %\u0026gt;%\rgroup_by(time) %\u0026gt;%\rsummarise(n = sum(counted_pedestrians)) %\u0026gt;%\rmutate(hour = lubridate::hour(time),\rdate = lubridate::as_date(time),\rday = lubridate::day(time),\rid = 1:length(n))\rplot2 \u0026lt;- df2 %\u0026gt;%\rfilter(time %in% time2) %\u0026gt;%\rgroup_by(time) %\u0026gt;%\rsummarise(n = sum(counted_pedestrians)) %\u0026gt;% mutate(hour = lubridate::hour(time),\rdate = lubridate::as_date(time),\rday = lubridate::day(time),\rid = 1:length(n))\rplot1 %\u0026gt;% ggplot(aes(x = id, y = n)) + geom_line(size = 1, color = \u0026quot;red\u0026quot;) +\rggtitle(\u0026quot;Pedestrians on a March weekend 2020 (red) versus 2019\u0026quot;,\rsubtitle = \u0026quot;Königstraße Stuttgart (Mitte)\u0026quot;) +\rylim(0, 15000)+\rylab(\u0026quot;Number of pedestrians per hour\u0026quot;) +\rxlab(\u0026quot;Time in hours (start: Friday 00:00)\u0026quot;) +\rgeom_line(data = plot2) +\rlabs(caption = \u0026quot;Source: hystreet.com\u0026quot;)\rThis is not the most insightful analysis since the figures that we can see were to expected given the current Covid-19 crisis. However, I think by using data from hystreet.com we can make a bit more evident how many people actually stayed away from the centres. I replicated the analysis for a couple of other cities of south west Germany – unsurprisingly, again, they all look pretty similar. If you continued up until this very end: Stay healthy – and please stay home (for now).\n","date":1585353600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585353600,"objectID":"8febba0930c8672cda23dadd54e2472f","permalink":"/post/see-german-shopping-streets-emptying/","publishdate":"2020-03-28T00:00:00Z","relpermalink":"/post/see-german-shopping-streets-emptying/","section":"post","summary":"The coronavirus is halting public live all over the world. So, many of the patterns of data regarding flight departures or traffic that we see these days are, objectively speaking, unsurprising. As of now, most of us are coping well with the situation and stay at home, as is recommended. As a result, the public sphere is emptying. Right now, this is a strategy that I support. However, it is hitting the economy harder than one might think.","tags":["english","R"],"title":"Covid-19 - See German shopping streets emptying","type":"post"},{"authors":[],"categories":["R"],"content":"\rAlmost a year ago, we (a team at CorrelAid) published our first open source package for R on CRAN: {newsanchor}. It queries the API of newsapi.org and allows you to easily download the articles relating to your search query of a range of popular international news outlets. Results include a variety of meta data, the URLs as well as (depending on whether you have a paid plan or not) parts of the article content. You can find all information needed reading its vignette. I also wrote an introductory blog post which you can find here.\nOne convenient use of {newsanchor} is to use it to scrape the articles’ content. Our package is of great help because it provides you with the corresponding URLs. It is fairly easy to build a scraper upon that. Here, I have to mention that vast parts of the following code stem from Jan Dix who, like me, co-authored the package. He wrote a scraper for the New York Times, it was originally to be included as a vignette of {newsanchor} (it had to be removed because of dependecy trouble, though). What I did was to build on his code and add another example for the popular German news magazine Spiegel.\n[Note: There is code for a progress bar in the following chunk. It is commented out so the R-Markdown output would be easier to read]\n# Load packages required\rlibrary(newsanchor) # download newspaper articles\rlibrary(robotstxt) # get robots.txt\rlibrary(httr) # http requests\rsuppressMessages(library(rvest)) # web scraping tools\rsuppressMessages(library(dplyr)) # easy data frame manipulation\rlibrary(stringr) # string/character manipulation # Get headlines published by SPON using newsanchor (example)\rresponse \u0026lt;- get_everything_all(query = \u0026quot;Merkel\u0026quot;,\rsources = \u0026quot;spiegel-online\u0026quot;,\rfrom = Sys.Date() - 3,\rto = Sys.Date())\r# Extract response data frame\rarticles \u0026lt;- response$results_df\r# Check robots.txt if scraping is OK\rsuppressMessages(allowed \u0026lt;- paths_allowed(articles$url))\rall(allowed)\r# Define parsing function\rget_article_body \u0026lt;- function (url) {\r# Download article page\rresponse \u0026lt;- GET(url)\r# Check if request was successful\rif (response$status_code != 200) return(NA)\r# Extract HTML\rhtml \u0026lt;- content(x = response, type = \u0026quot;text\u0026quot;, encoding = \u0026quot;UTF-8\u0026quot;)\r# Parse html\rparsed_html \u0026lt;- read_html(html) # Define paragraph DOM selector\rselector \u0026lt;- \u0026quot;div.clearfix p\u0026quot;\r# Parse content\rparsed_html %\u0026gt;% html_nodes(selector) %\u0026gt;% # extract all paragraphs within class \u0026#39;article-section\u0026#39;\rhtml_text() %\u0026gt;% # extract content of the \u0026lt;p\u0026gt; tags\rstr_replace_all(\u0026quot;\\n\u0026quot;, \u0026quot;\u0026quot;) %\u0026gt;% # replace all line breaks\rpaste(collapse = \u0026quot; \u0026quot;) # join all paragraphs into one string\r}\r# Apply function to all URLs\r# Create new text column\rarticles$body \u0026lt;- NA\r# Initialize progress bar\r# pb \u0026lt;- txtProgressBar(min = 1, # max = nrow(articles), # initial = 1, # style = 3)\r# Loop through articles and apply function\rfor (i in 1:nrow(articles)) {\r# Apply function to i in URLS\rarticles$body[i] \u0026lt;- get_article_body(articles$url[i])\r## Update progress bar\r# setTxtProgressBar(pb, i)\r# Sleep for 1 sec\rSys.sleep(1)\r}\rBased on the articles’ content, you can, for example, compute sentiment analyses. Jan shows you how to do that for the New York Times in what was formerly our vignette here. I hope this is useful for some of you.\n","date":1580083200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580083200,"objectID":"31d28307af0306672ccf15c245382e42","permalink":"/post/scraping-spiegel-articles-using-newsanchor/","publishdate":"2020-01-27T00:00:00Z","relpermalink":"/post/scraping-spiegel-articles-using-newsanchor/","section":"post","summary":"Almost a year ago, we (a team at CorrelAid) published our first open source package for R on CRAN: {newsanchor}. It queries the API of newsapi.org and allows you to easily download the articles relating to your search query of a range of popular international news outlets. Results include a variety of meta data, the URLs as well as (depending on whether you have a paid plan or not) parts of the article content.","tags":["R","english"],"title":"Scraping Spiegel articles using {newsanchor}","type":"post"},{"authors":[],"categories":["R"],"content":"\rFor about two years now, I measure the air quality (more specifically: fine dust) on my balcony. Up to now, the data were stored by an Open Data project and displayed (more or less hidden) in the internet – at least you would need to know the name or ID of the sensor to access the data as very basic graphs. Here you could also find aggregated air quality data for all areas of Stuttgart. Recently, I began storing the data into a database myself to be able to do my own calculations (and eventually create a Shiny App that would display nicer, interactive graphs). Here, I wanted to very quickly write down what I did and what is going on.\nThe sensor\rThe sensor measuring fine dust is a very basic one. All its components cost around 30€ and can be shopped online. Honestly, I did not build the sensor myself; a colleague had a spare one and gave it to me. However, I was told it is not too hard to build such a sensor. In fact, here you can find a (German only) manual for doing so, including a list of stuff you need for the sensor itself. My sensor is located on my balcony, which in turn faces a highly frequented road, where quite a lot of buses are passing by during the day. The sensor delivers the data it measures every (more or less) five minutes via a JSON file to an API that is open for everyone to read. The sensor itself runs quite smoothly, only recently I run into problems where it ceases to send information – this is fixed after quickly removing it from its power source.\n\rAmazon AWS with RStudio Server\rWhen I came up with the idea of automatically storing the data into a database, I was initially thinking about setting up a Raspberry Pi to do the job. Yet, a friend recommended using Amazon AWS instead. Frankly, when I did this, I was new to cloud computing. However, there are a lot of tutorials out there on how to set up an Amazon AWS instance. I remember choosing the smallest one (EC2) which is also completely free of charge. That is useful since it is supposed to run 24/7. Whilst setting up the instance, I also installed RStudio Server and Shiny Server. That would later allow me to work on R scripts on the instance in order to do the parsing of the JSON file.\n\rThe script\rThe R script is nothing more than a simple parser for the JSON file that the API publishes. As a first step, I set up a SQLite database on my AWS instance using the R package sqldf. It contains only three variables: the timestamp of the measurement and the two fine dust measurements that the sensor gives us. The script parses the JSON file (using jsonlite) and writes it into the database using SQL. Here is some example code:\ndb \u0026lt;- dbConnect(SQLite(), dbname = \u0026quot;/home/user/database.sqlite\u0026quot;)\rquery1 \u0026lt;- paste0(\u0026quot;INSERT INTO database VALUES (\u0026#39;\u0026quot;, time1, \u0026quot;\u0026#39;,\u0026quot;, PM_10_1, \u0026quot;,\u0026quot;, PM_25_1, \u0026quot;)\u0026quot;)\rdbSendQuery(conn = db, query1)\r\rSetting up the Crontab\rThe last step is to automate the R script that we have stored on the AWS instance, too. You can do this using a Crontab, which is an automation tool that is already present on the Linux of the EC2 instance. You can find a tutorial here.\n\r","date":1575504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575504000,"objectID":"27493ea66b9ef091d1a5586c1b1e20e7","permalink":"/post/automatically-filling-sqlite-db-using-aws/","publishdate":"2019-12-05T00:00:00Z","relpermalink":"/post/automatically-filling-sqlite-db-using-aws/","section":"post","summary":"For about two years now, I measure the air quality (more specifically: fine dust) on my balcony. Up to now, the data were stored by an Open Data project and displayed (more or less hidden) in the internet – at least you would need to know the name or ID of the sensor to access the data as very basic graphs. Here you could also find aggregated air quality data for all areas of Stuttgart.","tags":["english","R"],"title":"Automatically filling a SQLite database using AWS and RStudio Server","type":"post"},{"authors":[],"categories":["R"],"content":"\rAt CorrelAid, we developed a tool for communication scientists, journalists and data scientists alike: {newsanchor}, CorrelAid’s first open source R package. It conveniently helps you to access breaking news and articles from over 30,000 news sources and blogs - using the API of newsapi.org. I have co-authored the package and I am thrilled to introduce the package here, too.\nThe (mostly free) News API is one way to access text as a resource for data analyses. It provides news articles and breaking news from a variety of sources across various countries, delivered to the analyst via an API (HTTP REST). Users are offered three API endpoints: top headlines, everything, and sources. Top headlines provides access to live breaking headlines of the news sources in a given country. Everything outputs articles published by these sources on a specified search query, even back in the past. Sources helps users to get access to the set of news sources that are available to top headlines.\nAll search requests come with different meta data (URL, author, date of publication, topic, etc.) and can be refined by a huge variety of additional parameters (sources, topic, time, relevance, etc.). For more details, see newsapi.org. Note for German scientists and journalists: In Germany, the following sources are available: Spiegel Online, Handelsblatt, Tagesspiegel, Wired, Gründerszene, BILD, Zeit Online, Focus Online, t3n and Wirtschaftswoche.\nAfter a short registration, the API can be accessed via code: through client libraries such as JavaScript or Ruby. But until now, there has been no R package that does the work (or search) conveniently. Now, at CorrelAid, a team of five data analysts developed this package. The package is called {newsanchor} and is available on CRAN: install.packages(\"newsanchor\").\nNewsanchor provides three functions that correspond to the API’s endpoints: get_headlines(), get_everything() and get_sources(). They help users to conveniently scrape the resources of News API, specify different search parameters and obtain results as 1) a data frame with results and 2) a list with meta data on the search. We also provide comprehensive error messages to make troubleshooting easy. You find details on the usage of newsanchor and its core functions in our general CRAN vignette.\nAnother reason for us to develop the package was that analyses based on words are becoming increasingly important. Political scientists, for example, classify parties on any ideological dimension using party manifestos. Other scholars focus on news articles to extract the (political) framings of the texts. Using automatisation, it is, for example, possible to calculate the sentiment of a given text fragment such as, for instance, online commentaries. The resulting data prove useful both as a dependent variable as well as an independent variable of any further analysis.\nThe importance of text analyses arises from the origin of ‘texts’: People aim at a certain reaction of their readers. Among the producers of texts with most influence are the media: newspapers, online magazines or blogs. By publishing articles, opinion pieces and analyses, they shape public opinion. The topics they choose (or not choose), the words they use, the quantity of articles on a certain issue – all these factors make them a worthy basis of investigation.\nAs already mentioned, an example would be to calculate the sentiment of news articles. Newsanchor can help to filter and scrape texts from news sources. In our example code, our co-developer Jan Dix shows you how to do so by getting URLs of the New York Times with newsanchor::get_everything(), subsequently scraping them with {httr} and analysing the articles’ sentiments.\nWe hope {newsanchor} will help scientists, journalists and other data enthusiasts to start scraping and using text data based on news articles.\n","date":1559520000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559520000,"objectID":"1ab9d52091a2317ac52c8766692e377d","permalink":"/post/introducing-the-r-newsanchor-package/","publishdate":"2019-06-03T00:00:00Z","relpermalink":"/post/introducing-the-r-newsanchor-package/","section":"post","summary":"At CorrelAid, we developed a tool for communication scientists, journalists and data scientists alike: {newsanchor}, CorrelAid’s first open source R package. It conveniently helps you to access breaking news and articles from over 30,000 news sources and blogs - using the API of newsapi.org. I have co-authored the package and I am thrilled to introduce the package here, too.\nThe (mostly free) News API is one way to access text as a resource for data analyses.","tags":["R","english"],"title":"Introducing the R {newsanchor} package","type":"post"},{"authors":null,"categories":["political science","modelling"],"content":"\rFor my Master’s thesis, submitted in July 2017, I investigated the voting behaviour of the Scottish electorate in the 2014 Scottish independence referendum. More precisely, I was the first attempting to explain voting behaviour in the referendum based on a multivariate quantitative approach. In doing so, I additionally focused on Scots’ attitudes towards the European Union and how these attitudes influence(d) their behaviour at the ballots. Whilst the existing literature on the referendum mostly ignored the European dimension of the independence debate, my results suggest that the Scots did in fact somehow think about the future of Scotland within or outside of the EU when the cast their vote – and that these attitudes did have an effect on how they would decide.\nIn sum, my results are the following (ceteris paribus): Scots who found EU membership to be a good thing and who thought that an independent Scotland would not be able to remain in the EU had a comparatively low probability of voting in favour of independence. However, once they believed that independent Scotland would be able to remain an EU member, the probability of voting in favour of independence increased sharply. These effects (based on logistic regression controlling for all sorts of variables and attitudes such as age, gender, nationalism, party identificiation, etc.) become more easy to understand once they are presented in a graph using predicted probabilities (higher values of “Perceived Likelihood of Scotland retaining EU membership” mark higher confidence of the survey respondent):\rThis relationship gets even more pronounced after computing an interaction effect and separating between proponents and opponents of EU membership:\rThis blog post was just meant to be as short presentation of my results. Here you can download the PDF with the full thesis.\n","date":1546992000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546992000,"objectID":"60838c31ec457b538430c5c0cf293986","permalink":"/post/voting-behaviour-in-2014-scottish-independence-referendum/","publishdate":"2019-01-09T00:00:00Z","relpermalink":"/post/voting-behaviour-in-2014-scottish-independence-referendum/","section":"post","summary":"For my Master’s thesis, submitted in July 2017, I investigated the voting behaviour of the Scottish electorate in the 2014 Scottish independence referendum. More precisely, I was the first attempting to explain voting behaviour in the referendum based on a multivariate quantitative approach. In doing so, I additionally focused on Scots’ attitudes towards the European Union and how these attitudes influence(d) their behaviour at the ballots. Whilst the existing literature on the referendum mostly ignored the European dimension of the independence debate, my results suggest that the Scots did in fact somehow think about the future of Scotland within or outside of the EU when the cast their vote – and that these attitudes did have an effect on how they would decide.","tags":["political science"],"title":"Voting Behaviour in the 2014 Scottish Independence Referendum","type":"post"},{"authors":null,"categories":null,"content":"Angaben gemäß § 5 TMG \u0026amp; verantwortlich für den Inhalt nach § 55 Abs. 2 RStV:\nYannik Buhl Welfenstraße 50 70599 Stuttgart\nKontakt\nExterne Bilder \u0026amp; Logos Icons made by Freepik from Flaticon are licensed by CC 3.0 BY.\nDatenschutz / DSGVO Beim Besuch dieser Homepage werden von mir keine personenbezogen Daten verarbeitet oder gespeichert. Diese Homepage wird gehosted bei Netlify. Netlify speichert nach eigenen Angaben lediglich die IP-Adressen der Seitenbesucher für nicht mehr als 30 Tage. Mehr Informationen gibt es hier.\nTwitter Auf den Seiten sind Funktionen des Dienstes Twitter eingebunden. Diese Funktionen werden angeboten durch die Twitter Inc., 1355 Market Street, Suite 900, San Francisco, CA 94103, USA. Durch das Benutzen von Twitter und der Funktion „Re-Tweet“ werden die von Ihnen besuchten Webseiten mit Ihrem Twitter-Account verknüpft und anderen Nutzern bekannt gegeben. Dabei werden auch Daten an Twitter übertragen. Wir weisen darauf hin, dass wir als Anbieter der Seiten keine Kenntnis vom Inhalt der übermittelten Daten sowie deren Nutzung durch Twitter erhalten. Weitere Informationen hierzu finden Sie in der Datenschutzerklärung von Twitter unter https://twitter.com/privacy. Ihre Datenschutzeinstellungen bei Twitter können Sie in den Konto-Einstellungen unter: https://twitter.com/account/settings ändern.\nYouTube Die Webseite nutzt Plugins der von Google betriebenen Seite YouTube. Betreiber der Seiten ist die YouTube, LLC, 901 Cherry Ave., San Bruno, CA 94066, USA. Wenn Sie eine der mit einem YouTube-Plugin ausgestatteten Seiten besuchen, wird eine Verbindung zu den Servern von YouTube hergestellt. Dabei wird dem Youtube-Server mitgeteilt, welche unserer Seiten Sie besucht haben. Wenn Sie in Ihrem YouTube-Account eingeloggt sind ermöglichen Sie YouTube, Ihr Surfverhalten Ihrem persönlichen Profil zuzuordnen. Dies können Sie verhindern, indem Sie sich aus Ihrem YouTube-Account ausloggen. Weitere Informationen zum Umgang von Nutzerdaten finden Sie in der Datenschutzerklärung von YouTube unter: https://www.google.de/intl/de/policies/privacy\nSSL-Verschlüsselung Diese Seite nutzt zum Schutz der Übertragung vertraulicher Inhalte, wie zum Beispiel der Anfragen, die Sie an uns als Seitenbetreiber senden, eine SSL-Verschlüsselung. Eine verschlüsselte Verbindung erkennen Sie daran, dass die Adresszeile des Browsers von „http://“ auf „https://“ wechselt und an dem Schloss-Symbol in Ihrer Browserzeile. Wenn die SSL Verschlüsselung aktiviert ist, können die Daten, die Sie an uns übermitteln, nicht von Dritten mitgelesen werden.\nRecht auf Auskunft, Löschung, Sperrung Sie haben jederzeit das Recht auf unentgeltliche Auskunft über Ihre gespeicherten personenbezogenen Daten, deren Herkunft und Empfänger und den Zweck der Datenverarbeitung sowie ein Recht auf Berichtigung, Sperrung oder Löschung dieser Daten. Hierzu sowie zu weiteren Fragen zum Thema personenbezogene Daten können Sie sich jederzeit unter der im Impressum angegebenen Adresse melden.\nHaftungsausschluss Haftung für Inhalte Die Inhalte unserer Seiten wurden mit größter Sorgfalt erstellt. Für die Richtigkeit, Vollständigkeit und Aktualität der Inhalte können wir jedoch keine Gewähr übernehmen. Als Diensteanbieter sind wir gemäß § 7 Abs.1 TMG für eigene Inhalte auf diesen Seiten nach den allgemeinen Gesetzen verantwortlich. Nach §§ 8 bis 10 TMG sind wir als Diensteanbieter jedoch nicht verpflichtet, übermittelte oder gespeicherte fremde Informationen zu überwachen oder nach Umständen zu forschen, die auf eine rechtswidrige Tätigkeit hinweisen. Verpflichtungen zur Entfernung oder Sperrung der Nutzung von Informationen nach den allgemeinen Gesetzen bleiben hiervon unberührt. Eine diesbezügliche Haftung ist jedoch erst ab dem Zeitpunkt der Kenntnis einer konkreten Rechtsverletzung möglich. Bei Bekanntwerden von entsprechenden Rechtsverletzungen werden wir diese Inhalte umgehend entfernen.\nHaftung für Links Unser Angebot enthält Links zu externen Webseiten Dritter, auf deren Inhalte wir keinen Einfluss haben. Deshalb können wir für diese fremden Inhalte auch keine Gewähr übernehmen. Für die Inhalte der verlinkten Seiten ist stets der jeweilige Anbieter oder Betreiber der Seiten verantwortlich. Die verlinkten Seiten wurden zum Zeitpunkt der Verlinkung auf mögliche Rechtsverstöße überprüft. Rechtswidrige Inhalte waren zum Zeitpunkt der Verlinkung nicht erkennbar. Eine permanente inhaltliche Kontrolle der verlinkten Seiten ist jedoch ohne konkrete Anhaltspunkte einer Rechtsverletzung nicht zumutbar. Bei Bekanntwerden von Rechtsverletzungen werden wir derartige Links umgehend entfernen.\nUrheberrecht Die durch die Seitenbetreiber erstellten Inhalte und Werke auf diesen Seiten unterliegen dem deutschen Urheberrecht. Die Vervielfältigung, Bearbeitung, Verbreitung und jede Art der Verwertung außerhalb der Grenzen des Urheberrechtes bedürfen der schriftlichen Zustimmung des jeweiligen Autors bzw. Erstellers. Downloads und Kopien dieser Seite sind nur für den privaten, nicht kommerziellen Gebrauch gestattet. Soweit die Inhalte auf dieser Seite nicht vom Betreiber erstellt wurden, werden die Urheberrechte Dritter beachtet. Insbesondere werden Inhalte Dritter als solche gekennzeichnet. Sollten Sie trotzdem auf eine Urheberrechtsverletzung aufmerksam werden, bitten wir um einen entsprechenden Hinweis. Bei Bekanntwerden von Rechtsverletzungen werden wir derartige Inhalte umgehend entfernen.\nDatenschutz Die Nutzung unserer Webseite ist in der Regel ohne Angabe personenbezogener Daten möglich. Soweit auf unseren Seiten personenbezogene Daten (beispielsweise Name, Anschrift oder eMail-Adressen) erhoben werden, erfolgt dies, soweit möglich, stets auf freiwilliger Basis. Diese Daten werden ohne Ihre ausdrückliche Zustimmung nicht an Dritte weitergegeben. Wir weisen darauf hin, dass die Datenübertragung im Internet (z.B. bei der Kommunikation per E-Mail) Sicherheitslücken aufweisen kann. Ein lückenloser Schutz der Daten vor dem Zugriff durch Dritte ist nicht möglich. Der Nutzung von im Rahmen der Impressumspflicht veröffentlichten Kontaktdaten durch Dritte zur Übersendung von nicht ausdrücklich angeforderter Werbung und Informationsmaterialien wird hiermit ausdrücklich widersprochen. Die Betreiber der Seiten behalten sich ausdrücklich rechtliche Schritte im Falle der unverlangten Zusendung von Werbeinformationen, etwa durch Spam-Mails, vor.\nGoogle Analytics (derzeit inaktiv auf dieser Seite) Diese Website benutzt Google Analytics, einen Webanalysedienst der Google Inc. (\u0026ldquo;Google\u0026rdquo;). Google Analytics verwendet sog. \u0026ldquo;Cookies\u0026rdquo;, Textdateien, die auf Ihrem Computer gespeichert werden und die eine Analyse der Benutzung der Website durch Sie ermöglicht. Die durch den Cookie erzeugten Informationen über Ihre Benutzung dieser Website (einschließlich Ihrer IP-Adresse) wird an einen Server von Google in den USA übertragen und dort gespeichert. Google wird diese Informationen benutzen, um Ihre Nutzung der Website auszuwerten, um Reports über die Websiteaktivitäten für die Websitebetreiber zusammenzustellen und um weitere mit der Websitenutzung und der Internetnutzung verbundene Dienstleistungen zu erbringen. Auch wird Google diese Informationen gegebenenfalls an Dritte übertragen, sofern dies gesetzlich vorgeschrieben oder soweit Dritte diese Daten im Auftrag von Google verarbeiten. Google wird in keinem Fall Ihre IP-Adresse mit anderen Daten der Google in Verbindung bringen. Sie können die Installation der Cookies durch eine entsprechende Einstellung Ihrer Browser Software verhindern; wir weisen Sie jedoch darauf hin, dass Sie in diesem Fall gegebenenfalls nicht sämtliche Funktionen dieser Website voll umfänglich nutzen können. Durch die Nutzung dieser Website erklären Sie sich mit der Bearbeitung der über Sie erhobenen Daten durch Google in der zuvor beschriebenen Art und Weise und zu dem zuvor benannten Zweck einverstanden.\nImpressum erstellt mithilfe des Impressum Generators der Kanzlei Hasselbach, Rechtsanwälte für Arbeitsrecht und Familienrecht\n","date":1537221600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537221600,"objectID":"18d05a63a1c8d7ed973cc51838494e41","permalink":"/privacy/","publishdate":"2018-09-18T00:00:00+02:00","relpermalink":"/privacy/","section":"","summary":"Angaben gemäß § 5 TMG \u0026amp; verantwortlich für den Inhalt nach § 55 Abs. 2 RStV:\nYannik Buhl Welfenstraße 50 70599 Stuttgart\nKontakt\nExterne Bilder \u0026amp; Logos Icons made by Freepik from Flaticon are licensed by CC 3.0 BY.\nDatenschutz / DSGVO Beim Besuch dieser Homepage werden von mir keine personenbezogen Daten verarbeitet oder gespeichert. Diese Homepage wird gehosted bei Netlify. Netlify speichert nach eigenen Angaben lediglich die IP-Adressen der Seitenbesucher für nicht mehr als 30 Tage.","tags":null,"title":"Impressum/Datenschutz","type":"page"},{"authors":null,"categories":null,"content":"Im August 2014 bin ich am Frankfurter Flughafen ins Flugzeug gestiegen und nach Quito, Ecuador auf 2950 Metern über dem Meeresspiegel geflogen. Dort habe ich für einige Monate im Büro der dortigen Friedrich-Ebert-Stiftung zu arbeiten. Während meiner Zeit dort habe ich ein wundervolles Land gesehen, ich habe die Galápagos-Inseln besucht und die Nebelwälder am Rande des Amazonas. Außerdem habe ich Ausflüge nach Machu Picchu, Peru unternommen und auf dem Rückweg zwei Wochen lang Kubas Westen bereist. Die folgenden Bilder sind eine Auswahl derer, die dort (dortmals noch mit meiner Canon 500D) entstanden sind.\n\r\r\r\r\r\r ","date":1536530400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536530400,"objectID":"7b90b0cc45a6f86049d780f96c46e9ec","permalink":"/tutorial/ecuador/","publishdate":"2018-09-10T00:00:00+02:00","relpermalink":"/tutorial/ecuador/","section":"tutorial","summary":"Im August 2014 bin ich am Frankfurter Flughafen ins Flugzeug gestiegen und nach Quito, Ecuador auf 2950 Metern über dem Meeresspiegel geflogen. Dort habe ich für einige Monate im Büro der dortigen Friedrich-Ebert-Stiftung zu arbeiten. Während meiner Zeit dort habe ich ein wundervolles Land gesehen, ich habe die Galápagos-Inseln besucht und die Nebelwälder am Rande des Amazonas. Außerdem habe ich Ausflüge nach Machu Picchu, Peru unternommen und auf dem Rückweg zwei Wochen lang Kubas Westen bereist.","tags":null,"title":"Ecuador","type":"docs"},{"authors":null,"categories":null,"content":"Im Herbst 2017 bin ich mit einem Freund von mir in den Schwarzwald gefahren. Unser Ziel war es, so viel wie möglich von der wunderbaren Landschaft dieses Fleckchen Erdes in Baden-Württemberg zu fotografieren. Unsere Route führte uns am Ende in etwas weniger als einer Woche vom Nordschwarzwald (Hornisgrinde) bis in den Süden (Feldberg/Titisee). Auf dem Weg haben wir vor allem viele Wasserfälle besucht und mit unseren Neutraldichtefiltern experimentiert. Die Zahl an Wasserfällen war schier endlos: Triberger Wasserfälle, Zweribachwasserfälle, Burgbachwasserfall, Wasserfälle Allerheiligen\u0026hellip; Die folgenden Bilder sind alle auf dieser Reise entstanden.\n\r\r\r\r\r ","date":1536444000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536444000,"objectID":"f40037c7b4d3214b5022f17b02973a5d","permalink":"/tutorial/schwarzwald/","publishdate":"2018-09-09T00:00:00+02:00","relpermalink":"/tutorial/schwarzwald/","section":"tutorial","summary":"Im Herbst 2017 bin ich mit einem Freund von mir in den Schwarzwald gefahren. Unser Ziel war es, so viel wie möglich von der wunderbaren Landschaft dieses Fleckchen Erdes in Baden-Württemberg zu fotografieren. Unsere Route führte uns am Ende in etwas weniger als einer Woche vom Nordschwarzwald (Hornisgrinde) bis in den Süden (Feldberg/Titisee). Auf dem Weg haben wir vor allem viele Wasserfälle besucht und mit unseren Neutraldichtefiltern experimentiert. Die Zahl an Wasserfällen war schier endlos: Triberger Wasserfälle, Zweribachwasserfälle, Burgbachwasserfall, Wasserfälle Allerheiligen\u0026hellip; Die folgenden Bilder sind alle auf dieser Reise entstanden.","tags":null,"title":"Schwarzwald","type":"docs"},{"authors":[],"categories":null,"content":"Click on the Slides button above to view the built-in slides feature.\nSlides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using url_slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1483225200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483225200,"objectID":"cd6d9d084287506b4668ad90c6aff50a","permalink":"/talk/example-talk/","publishdate":"2017-01-01T00:00:00+01:00","relpermalink":"/talk/example-talk/","section":"talk","summary":"Click on the Slides button above to view the built-in slides feature.\nSlides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using url_slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461708000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461708000,"objectID":"80be3c9fcc86014efab0cec0f14957f6","permalink":"/project/deep-learning/","publishdate":"2016-04-27T00:00:00+02:00","relpermalink":"/project/deep-learning/","section":"project","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit.","tags":["Deep Learning"],"title":"Deep Learning","type":"project"},{"authors":null,"categories":null,"content":"","date":1461708000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461708000,"objectID":"553a94c5dfd3b8b099d8a12b2d248093","permalink":"/project/example-external-project/","publishdate":"2016-04-27T00:00:00+02:00","relpermalink":"/project/example-external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":["GA Cushen"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1441058400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441058400,"objectID":"5cec0ce6e082b377c504bc66cdf990c5","permalink":"/publication/person-re-identification/","publishdate":"2015-09-01T00:00:00+02:00","relpermalink":"/publication/person-re-identification/","section":"publication","summary":"Person re-identification is a critical security task for recognizing a person across spatially disjoint sensors. Previous work can be computationally intensive and is mainly based on low-level cues extracted from RGB data and implemented on a PC for a fixed sensor network (such as traditional CCTV). We present a practical and efficient framework for mobile devices (such as smart phones and robots) where high-level semantic soft biometrics are extracted from RGB and depth data. By combining these cues, our approach attempts to provide robustness to noise, illumination, and minor variations in clothing. This mobile approach may be particularly useful for the identification of persons in areas ill-served by fixed sensors or for tasks where the sensor position and direction need to dynamically adapt to a target. Results on the BIWI dataset are preliminary but encouraging. Further evaluation and demonstration of the system will be available on our website.","tags":[],"title":"A Person Re-Identification System For Mobile Devices","type":"publication"},{"authors":["GA Cushen","MS Nixon"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1372629600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372629600,"objectID":"caae70970030052c8f733b2ca8421a2b","permalink":"/publication/clothing-search/","publishdate":"2013-07-01T00:00:00+02:00","relpermalink":"/publication/clothing-search/","section":"publication","summary":"We present a mobile visual clothing search system whereby a smart phone user can either choose a social networking photo or take a new photo of a person wearing clothing of interest and search for similar clothing in a retail database. From the query image, the person is detected, clothing is segmented, and clothing features are extracted and quantized. The information is sent from the phone client to a server, where the feature vector of the query image is used to retrieve similar clothing products from online databases. The phone's GPS location is used to re-rank results by retail store location. State of the art work focuses primarily on the recognition of a diverse range of clothing offline and pays little attention to practical applications. Evaluated on a challenging dataset, the system is relatively fast and achieves promising results.","tags":[],"title":"Mobile visual clothing search","type":"publication"},{"authors":null,"categories":null,"content":"Welcome to Slides Academic\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34;\rif porridge == \u0026#34;blueberry\u0026#34;:\rprint(\u0026#34;Eating...\u0026#34;)\r Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}}\r{{% fragment %}} **Two** {{% /fragment %}}\r{{% fragment %}} Three {{% /fragment %}}\rPress Space to play!\nOne Two Three  A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}}\r- Only the speaker can read these notes\r- Press `S` key to view\r{{% /speaker_note %}}\rPress the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view \r  Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/img/boards.jpg\u0026#34; \u0026gt;}}\r{{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}}\r{{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}}\r Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1,\r.reveal section h2,\r.reveal section h3 {\rcolor: navy;\r}\r Questions? Ask\nDocumentation\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c2915ec5da95791851caafdcba9664af","permalink":"/slides/example-slides/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example-slides/","section":"slides","summary":"Welcome to Slides Academic\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34;\rif porridge == \u0026#34;blueberry\u0026#34;:\rprint(\u0026#34;Eating.","tags":null,"title":"Slides","type":"slides"}]